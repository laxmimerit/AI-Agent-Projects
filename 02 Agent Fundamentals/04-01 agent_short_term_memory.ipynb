{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Agent Short-Term Memory\n",
    "\n",
    "Short-term memory stores conversation history within a session using checkpointers.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Checkpointers persist conversation history\n",
    "- SQLite for development, PostgreSQL for production\n",
    "- Thread IDs manage separate sessions\n",
    "- Access agent state in tools with ToolRuntime\n",
    "- Modify agent state from tools for context offloading\n",
    "- Save/load conversation summaries for long conversations\n",
    "\n",
    "## Checkpointer Comparison\n",
    "\n",
    "| Type | Use Case | Setup |\n",
    "|------|----------|-------|\n",
    "| **SQLite** | Development, testing | Simple file-based |\n",
    "| **PostgreSQL** | Production, multi-user | Database connection |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage, ToolMessage, SystemMessage\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from pydantic import BaseModel\n",
    "from pathlib import Path\n",
    "from langgraph.types import Command\n",
    "from scripts import base_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Problem: No Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to personal information about you, including your name. I don't know who you are.\\n\\nIf you'd like me to refer to you by a specific name during our conversation, feel free to tell me!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = create_agent(model=model)\n",
    "\n",
    "agent.invoke({'messages': [HumanMessage(\"My name is John\")]})\n",
    "response = agent.invoke({'messages': [HumanMessage(\"What's my name?\")]})\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Short-Term Memory: SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "\n",
    "os.makedirs(\"db\", exist_ok=True)\n",
    "\n",
    "conn = sqlite3.connect(\"db/checkpoints.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    checkpointer=checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have memory of past conversations. Could you please tell me your name again?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"user_123\"}}\n",
    "\n",
    "agent.invoke({'messages': [HumanMessage(\"My name is John\")]}, config)\n",
    "response = agent.invoke({'messages': [HumanMessage(\"What's my name?\")]}, config)\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20992531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='My name is John', additional_kwargs={}, response_metadata={}, id='41f29461-4ba2-4ec3-8e3d-94a88e10acc1'),\n",
       "  AIMessage(content='Hello John! How can I help you today?', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019ba176-c2ad-7450-b4ae-9803ec00dd25-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 206, 'output_tokens': 10, 'total_tokens': 216, 'input_token_details': {'cache_read': 0}}),\n",
       "  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='a5896699-f58b-4611-9b78-d155793343e0'),\n",
       "  AIMessage(content=\"I'm sorry, I don't have memory of past conversations. Could you please tell me your name again?\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019ba176-c64b-7123-98b1-708eda3b118d-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 224, 'output_tokens': 24, 'total_tokens': 248, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Short-Term Memory: PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.checkpoint.postgres import PostgresSaver\n",
    "# import psycopg\n",
    "\n",
    "# with PostgresSaver.from_conn_string(os.getenv(\"POSTGRESQL_URL\")) as checkpointer:\n",
    "#     agent = create_agent(\n",
    "#         model=model,\n",
    "#         tools=[base_tools.web_search, base_tools.get_weather],\n",
    "#         checkpointer=checkpointer\n",
    "#     )\n",
    "    \n",
    "#     config = {\"configurable\": {\"thread_id\": \"postgres_session\"}}\n",
    "    \n",
    "#     agent.invoke({'messages': [HumanMessage(\"My city is Mumbai\")]}, config)\n",
    "#     response = agent.invoke({'messages': [HumanMessage(\"What's my city?\")]}, config)\n",
    "    \n",
    "#     response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Context Offloading: Read State in Tools\n",
    "\n",
    "Access agent state to save conversation summaries for context management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomContext(BaseModel):\n",
    "    user_id: str\n",
    "    thread_id: str\n",
    "\n",
    "@tool\n",
    "def save_conversation_summary(summary: str, runtime: ToolRuntime[CustomContext]) -> str:\n",
    "    \"\"\"Save conversation summary to disk for context offloading.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    thread_id = runtime.context.thread_id\n",
    "    \n",
    "    # Create directory structure\n",
    "    summary_dir = Path(f\"data/{user_id}/{thread_id}\")\n",
    "    summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Write summary\n",
    "    summary_path = summary_dir / \"summary.md\"\n",
    "    summary_path.write_text(summary)\n",
    "    \n",
    "    return f\"Summary saved to {summary_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test save summary\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[save_conversation_summary],\n",
    "    checkpointer=checkpointer,\n",
    "    context_schema=CustomContext\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"session1\"}}\n",
    "\n",
    "response = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"Save this summary: User discussed Python and AI topics\")]\n",
    "}, config=config, context=CustomContext(user_id=\"user_123\", thread_id=\"session1\"))\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Context Offloading: Modify State in Tools\n",
    "\n",
    "Load previous summaries and inject them into agent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def load_conversation_summary(runtime: ToolRuntime[CustomContext]) -> Command:\n",
    "    \"\"\"Load previous conversation summary from disk.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    thread_id = runtime.context.thread_id\n",
    "    \n",
    "    summary_path = Path(f\"data/{user_id}/{thread_id}/summary.md\")\n",
    "    \n",
    "    if not summary_path.exists():\n",
    "        return Command(update={\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    \"No previous summary found.\",\n",
    "                    tool_call_id=runtime.tool_call_id\n",
    "                )\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    # Read summary\n",
    "    summary_text = summary_path.read_text()\n",
    "    \n",
    "    # Update state with summary as system message\n",
    "    return Command(update={\n",
    "        \"messages\": [\n",
    "            SystemMessage(f\"Previous conversation summary:\\n{summary_text}\"),\n",
    "            ToolMessage(\n",
    "                \"Successfully loaded previous summary.\",\n",
    "                tool_call_id=runtime.tool_call_id\n",
    "            )\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test load summary\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[load_conversation_summary],\n",
    "    checkpointer=checkpointer,\n",
    "    context_schema=CustomContext\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"session1\"}}\n",
    "\n",
    "response = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"Load my previous conversation summary\")]\n",
    "}, config=config, context=CustomContext(user_id=\"user_123\", thread_id=\"session1\"))\n",
    "\n",
    "# Check messages - should have SystemMessage with summary\n",
    "for msg in response['messages']:\n",
    "    print(f\"{type(msg).__name__}: {msg.content[:100] if hasattr(msg, 'content') else msg}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Combined: Save and Load Context\n",
    "\n",
    "Real-world example combining both tools for context management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent with both save and load\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[save_conversation_summary, load_conversation_summary],\n",
    "    checkpointer=checkpointer,\n",
    "    context_schema=CustomContext\n",
    ")\n",
    "\n",
    "# Session 1: Have conversation and save summary\n",
    "config = {\"configurable\": {\"thread_id\": \"project_alpha\"}}\n",
    "\n",
    "agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"We're building a chatbot using LangChain\")]\n",
    "}, config=config, context=CustomContext(user_id=\"user_456\", thread_id=\"project_alpha\"))\n",
    "\n",
    "agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"Save summary: Building LangChain chatbot with memory and tools\")]\n",
    "}, config=config, context=CustomContext(user_id=\"user_456\", thread_id=\"project_alpha\"))\n",
    "\n",
    "print(\"Summary saved for project_alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session 2: Load summary in new thread\n",
    "config = {\"configurable\": {\"thread_id\": \"project_alpha_review\"}}\n",
    "\n",
    "response = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"Load the project_alpha summary and tell me what we were working on\")]\n",
    "}, config=config, context=CustomContext(user_id=\"user_456\", thread_id=\"project_alpha\"))\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create your own context offloading tool\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
