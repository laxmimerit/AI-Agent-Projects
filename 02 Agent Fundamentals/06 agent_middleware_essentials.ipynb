{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Middleware Essentials\n",
    "\n",
    "Add production-ready middleware for summarization, limits, fallbacks, and dynamic prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "from scripts import base_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')\n",
    "\n",
    "# Setup checkpointer\n",
    "conn = sqlite3.connect(\"data/middleware_agent.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline agent without middleware\n",
    "basic_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search, base_tools.get_weather],\n",
    "    checkpointer=checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SummarizationMiddleware\n",
    "\n",
    "Automatically compress long conversations when history exceeds threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "agent_summary = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    checkpointer=checkpointer,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=ChatGoogleGenerativeAI(model='gemini-2.5-flash'),\n",
    "            trigger=[(\"messages\", 15)],  # Summarize when > 15 messages\n",
    "            keep=(\"messages\", 5)  # Keep last 5 unsummarized\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "config = {'configurable': {'thread_id': 'summary_session'}}\n",
    "response = agent_summary.invoke({\n",
    "    'messages': [HumanMessage(\n",
    "        \"Search for Apple, Microsoft, and Tesla stock news\"\n",
    "    )]\n",
    "}, config)\n",
    "\n",
    "len(response['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCallLimitMiddleware\n",
    "\n",
    "Prevent runaway costs by limiting model calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
    "\n",
    "agent_limit = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[\n",
    "        ModelCallLimitMiddleware(\n",
    "            run_limit=2,  # Max 2 model calls\n",
    "            exit_behavior=\"end\"  # Stop when limit reached\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent_limit.invoke({\n",
    "    'messages': [HumanMessage(\"Search for news on 5 different companies\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToolCallLimitMiddleware\n",
    "\n",
    "Limit tool executions to manage API usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ToolCallLimitMiddleware\n",
    "\n",
    "agent_tool_limit = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[\n",
    "        ToolCallLimitMiddleware(\n",
    "            run_limit=2,\n",
    "            exit_behavior=\"continue\"  # Continue without more tools\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent_tool_limit.invoke({\n",
    "    'messages': [HumanMessage(\"Search for Apple, Microsoft, and Google news\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelFallbackMiddleware\n",
    "\n",
    "Fallback to alternate model on failure or for cost optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ModelFallbackMiddleware\n",
    "\n",
    "fallback_model = ChatGoogleGenerativeAI(model='gemini-2.0-flash-exp')\n",
    "\n",
    "agent_fallback = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[ModelFallbackMiddleware(fallback_model)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic System Prompt\n",
    "\n",
    "Modify system prompt based on runtime context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "class Context(TypedDict):\n",
    "    user_role: str\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_role_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"Generate system prompt based on user role.\"\"\"\n",
    "    user_role = request.runtime.context.get(\"user_role\", \"user\")\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "    \n",
    "    if user_role == \"expert\":\n",
    "        return f\"{base_prompt} Provide detailed technical responses.\"\n",
    "    elif user_role == \"beginner\":\n",
    "        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "agent_dynamic = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[user_role_prompt],\n",
    "    context_schema=Context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with expert context\n",
    "expert_response = agent_dynamic.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n",
    "    context={\"user_role\": \"expert\"}\n",
    ")\n",
    "\n",
    "expert_response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with beginner context\n",
    "beginner_response = agent_dynamic.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n",
    "    context={\"user_role\": \"beginner\"}\n",
    ")\n",
    "\n",
    "beginner_response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Multiple Middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production agent with stacked middleware\n",
    "agent_production = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    checkpointer=checkpointer,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=ChatGoogleGenerativeAI(model='gemini-2.5-flash'),\n",
    "            trigger=[(\"messages\", 15)],\n",
    "            keep=(\"messages\", 5)\n",
    "        ),\n",
    "        ModelCallLimitMiddleware(run_limit=3, exit_behavior=\"end\"),\n",
    "        ToolCallLimitMiddleware(run_limit=3, exit_behavior=\"continue\"),\n",
    "        ModelFallbackMiddleware(fallback_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "config = {'configurable': {'thread_id': 'production'}}\n",
    "response = agent_production.invoke({\n",
    "    'messages': [HumanMessage(\"Analyze tech sector trends\")]\n",
    "}, config)\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Middleware adds production capabilities without changing agent logic\n",
    "- SummarizationMiddleware prevents context overflow\n",
    "- Limits control costs and API usage\n",
    "- Fallbacks improve reliability\n",
    "- Dynamic prompts enable context-aware behavior\n",
    "- Middleware execution order matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create custom middleware configuration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
