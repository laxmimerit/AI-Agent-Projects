{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Middleware Essentials\n",
    "\n",
    "Add production-ready middleware for message management, limits, fallbacks, and dynamic prompts.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Middleware adds production capabilities without changing agent logic\n",
    "- Trim messages keeps recent messages within context window\n",
    "- Delete messages removes specific or all messages from state\n",
    "- SummarizationMiddleware prevents context overflow with summaries\n",
    "- TodoListMiddleware provides task planning and tracking\n",
    "- Limits control costs and API usage\n",
    "- Fallbacks improve reliability\n",
    "- Dynamic prompts enable context-aware behavior\n",
    "- ShellToolMiddleware enables command execution\n",
    "- FilesystemFileSearchMiddleware provides file search capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "from scripts import base_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')\n",
    "\n",
    "# Setup checkpointer\n",
    "conn = sqlite3.connect(\"db/middleware_agent.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "checkpointer.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim Messages\n",
    "\n",
    "Keep only recent messages to fit context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langchain.agents import AgentState\n",
    "from langchain.agents.middleware import before_model\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any\n",
    "\n",
    "@before_model\n",
    "def trim_messages(state: AgentState, runtime: Runtime):\n",
    "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if len(messages) <= 3:\n",
    "        return None  # No changes needed\n",
    "\n",
    "    first_msg = messages[0]\n",
    "    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n",
    "    new_messages = [first_msg] + recent_messages\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "            *new_messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[trim_messages],\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"trim_session\"}}\n",
    "\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Messages\n",
    "\n",
    "Remove specific messages or clear entire history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import after_model\n",
    "\n",
    "@after_model\n",
    "def delete_old_messages(state: AgentState, runtime: Runtime):\n",
    "    \"\"\"Remove old messages to keep conversation manageable.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 2:\n",
    "        # Remove the earliest two messages\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "    return None\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[delete_old_messages],\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"delete_session\"}}\n",
    "\n",
    "agent.invoke({\"messages\": \"hi! I'm alice\"}, config)\n",
    "response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SummarizationMiddleware\n",
    "\n",
    "Automatically compress long conversations using summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    checkpointer=checkpointer,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=ChatGoogleGenerativeAI(model='gemini-2.5-flash'),\n",
    "            trigger=[(\"messages\", 15)],  # Summarize when > 15 messages\n",
    "            keep=(\"messages\", 5)  # Keep last 5 unsummarized\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "config = {'configurable': {'thread_id': 'summary_session'}}\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\n",
    "        \"Search for Apple, Microsoft, and Tesla stock news\"\n",
    "    )]\n",
    "}, config)\n",
    "\n",
    "len(response['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TodoListMiddleware\n",
    "\n",
    "Equip agents with task planning and tracking for complex multi-step tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import TodoListMiddleware\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def read_file(path: str):\n",
    "    \"\"\"Read file contents.\"\"\"\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {e}\"\n",
    "\n",
    "@tool\n",
    "def write_file(path: str, content: str):\n",
    "    \"\"\"Write content to file.\"\"\"\n",
    "    try:\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "        return f\"Successfully wrote to {path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing file: {e}\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[read_file, write_file],\n",
    "    middleware=[TodoListMiddleware()],\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "config = {'configurable': {'thread_id': 'todo_session'}}\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\n",
    "        \"Create a new file called test.txt with 'Hello World', then read it back\"\n",
    "    )]\n",
    "}, config)\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCallLimitMiddleware\n",
    "\n",
    "Prevent runaway costs by limiting model calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[\n",
    "        ModelCallLimitMiddleware(\n",
    "            run_limit=2,  # Max 2 model calls\n",
    "            exit_behavior=\"end\"  # Stop when limit reached\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Search for news on 5 different companies\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToolCallLimitMiddleware\n",
    "\n",
    "Limit tool executions to manage API usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ToolCallLimitMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[\n",
    "        ToolCallLimitMiddleware(\n",
    "            run_limit=2,\n",
    "            exit_behavior=\"continue\"  # Continue without more tools\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Search for Apple, Microsoft, and Google news\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelFallbackMiddleware\n",
    "\n",
    "Fallback to alternate model on failure or for cost optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ModelFallbackMiddleware\n",
    "\n",
    "fallback_model = ChatGoogleGenerativeAI(model='gemini-2.0-flash-exp')\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[ModelFallbackMiddleware(fallback_model)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic System Prompt\n",
    "\n",
    "Modify system prompt based on runtime context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "class Context(TypedDict):\n",
    "    user_role: str\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_role_prompt(request: ModelRequest):\n",
    "    \"\"\"Generate system prompt based on user role.\"\"\"\n",
    "    user_role = request.runtime.context.get(\"user_role\", \"user\")\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "    \n",
    "    if user_role == \"expert\":\n",
    "        return f\"{base_prompt} Provide detailed technical responses.\"\n",
    "    elif user_role == \"beginner\":\n",
    "        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[user_role_prompt],\n",
    "    context_schema=Context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with expert context\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n",
    "    context={\"user_role\": \"expert\"}\n",
    ")\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with beginner context\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n",
    "    context={\"user_role\": \"beginner\"}\n",
    ")\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShellToolMiddleware\n",
    "\n",
    "Expose persistent shell session for command execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ShellToolMiddleware, HostExecutionPolicy\n",
    "\n",
    "# Basic shell with host execution\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ShellToolMiddleware(\n",
    "            workspace_root=\"./workspace\",\n",
    "            execution_policy=HostExecutionPolicy(),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"List files in the current directory\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shell with startup commands\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ShellToolMiddleware(\n",
    "            workspace_root=\"./workspace\",\n",
    "            startup_commands=[\"echo 'Shell initialized'\"],\n",
    "            execution_policy=HostExecutionPolicy(),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Create a directory called 'test_dir'\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FilesystemFileSearchMiddleware\n",
    "\n",
    "Provide Glob and Grep search tools over filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import FilesystemFileSearchMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        FilesystemFileSearchMiddleware(\n",
    "            root_path=\"../\",\n",
    "            use_ripgrep=True,\n",
    "            max_file_size_mb=10,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Find all Python files in this directory\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for specific content\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Find files containing 'create_agent'\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Multiple Middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production agent with stacked middleware\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    checkpointer=checkpointer,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=ChatGoogleGenerativeAI(model='gemini-2.5-flash'),\n",
    "            trigger=[(\"messages\", 15)],\n",
    "            keep=(\"messages\", 5)\n",
    "        ),\n",
    "        TodoListMiddleware(),\n",
    "        ModelCallLimitMiddleware(run_limit=3, exit_behavior=\"end\"),\n",
    "        ToolCallLimitMiddleware(run_limit=3, exit_behavior=\"continue\"),\n",
    "        ModelFallbackMiddleware(fallback_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "config = {'configurable': {'thread_id': 'production'}}\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Analyze tech sector trends\")]\n",
    "}, config)\n",
    "\n",
    "response['messages'][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
