{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Middleware Essentials\n",
    "\n",
    "Add production-ready middleware for message management, limits, fallbacks, and dynamic prompts.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Middleware adds production capabilities without changing agent logic\n",
    "- Trim messages keeps recent messages within context window\n",
    "- Delete messages removes specific or all messages from state\n",
    "- SummarizationMiddleware prevents context overflow with summaries\n",
    "- Limits control costs and API usage\n",
    "- Fallbacks improve reliability\n",
    "- Dynamic prompts enable context-aware behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "from scripts import base_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')\n",
    "\n",
    "# Setup checkpointer\n",
    "conn = sqlite3.connect(\"db/middleware_agent.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "checkpointer.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim Messages\n",
    "\n",
    "Keep only recent messages to fit context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langchain.agents import AgentState\nfrom langchain.agents.middleware import before_model\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime):\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return None  # No changes needed\n\n    first_msg = messages[0]\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n    new_messages = [first_msg] + recent_messages\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *new_messages\n        ]\n    }\n\nagent = create_agent(\n    model=model,\n    tools=[],\n    middleware=[trim_messages],\n    checkpointer=checkpointer\n)\n\nconfig = {\"configurable\": {\"thread_id\": \"trim_session\"}}\n\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nresponse = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nresponse['messages'][-1].content"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Messages\n",
    "\n",
    "Remove specific messages or clear entire history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain.agents.middleware import after_model\n\n@after_model\ndef delete_old_messages(state: AgentState, runtime: Runtime):\n    \"\"\"Remove old messages to keep conversation manageable.\"\"\"\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # Remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n    return None\n\nagent = create_agent(\n    model=model,\n    tools=[],\n    middleware=[delete_old_messages],\n    checkpointer=checkpointer\n)\n\nconfig = {\"configurable\": {\"thread_id\": \"delete_session\"}}\n\nagent.invoke({\"messages\": \"hi! I'm alice\"}, config)\nresponse = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nresponse['messages'][-1].content"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SummarizationMiddleware\n",
    "\n",
    "Automatically compress long conversations using summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    checkpointer=checkpointer,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=ChatGoogleGenerativeAI(model='gemini-2.5-flash'),\n",
    "            trigger=[(\"messages\", 15)],  # Summarize when > 15 messages\n",
    "            keep=(\"messages\", 5)  # Keep last 5 unsummarized\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "config = {'configurable': {'thread_id': 'summary_session'}}\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\n",
    "        \"Search for Apple, Microsoft, and Tesla stock news\"\n",
    "    )]\n",
    "}, config)\n",
    "\n",
    "len(response['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCallLimitMiddleware\n",
    "\n",
    "Prevent runaway costs by limiting model calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[\n",
    "        ModelCallLimitMiddleware(\n",
    "            run_limit=2,  # Max 2 model calls\n",
    "            exit_behavior=\"end\"  # Stop when limit reached\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Search for news on 5 different companies\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToolCallLimitMiddleware\n",
    "\n",
    "Limit tool executions to manage API usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ToolCallLimitMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[\n",
    "        ToolCallLimitMiddleware(\n",
    "            run_limit=2,\n",
    "            exit_behavior=\"continue\"  # Continue without more tools\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Search for Apple, Microsoft, and Google news\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelFallbackMiddleware\n",
    "\n",
    "Fallback to alternate model on failure or for cost optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ModelFallbackMiddleware\n",
    "\n",
    "fallback_model = ChatGoogleGenerativeAI(model='gemini-2.0-flash-exp')\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    middleware=[ModelFallbackMiddleware(fallback_model)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic System Prompt\n",
    "\n",
    "Modify system prompt based on runtime context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import TypedDict\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\nclass Context(TypedDict):\n    user_role: str\n\n@dynamic_prompt\ndef user_role_prompt(request: ModelRequest):\n    \"\"\"Generate system prompt based on user role.\"\"\"\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\n    base_prompt = \"You are a helpful assistant.\"\n    \n    if user_role == \"expert\":\n        return f\"{base_prompt} Provide detailed technical responses.\"\n    elif user_role == \"beginner\":\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n    \n    return base_prompt\n\nagent = create_agent(\n    model=model,\n    tools=[base_tools.web_search],\n    middleware=[user_role_prompt],\n    context_schema=Context\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with expert context\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n",
    "    context={\"user_role\": \"expert\"}\n",
    ")\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with beginner context\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n",
    "    context={\"user_role\": \"beginner\"}\n",
    ")\n",
    "\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Multiple Middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production agent with stacked middleware\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[base_tools.web_search],\n",
    "    checkpointer=checkpointer,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=ChatGoogleGenerativeAI(model='gemini-2.5-flash'),\n",
    "            trigger=[(\"messages\", 15)],\n",
    "            keep=(\"messages\", 5)\n",
    "        ),\n",
    "        ModelCallLimitMiddleware(run_limit=3, exit_behavior=\"end\"),\n",
    "        ToolCallLimitMiddleware(run_limit=3, exit_behavior=\"continue\"),\n",
    "        ModelFallbackMiddleware(fallback_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "config = {'configurable': {'thread_id': 'production'}}\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Analyze tech sector trends\")]\n",
    "}, config)\n",
    "\n",
    "response['messages'][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}