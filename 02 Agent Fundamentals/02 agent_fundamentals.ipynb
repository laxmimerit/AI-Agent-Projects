{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Fundamentals\n",
    "\n",
    "Learn the core concepts of building AI agents with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an AI Agent?\n",
    "\n",
    "An agent combines a language model with tools to create systems that reason, decide, and work towards solutions iteratively.\n",
    "\n",
    "**Core Components:**\n",
    "1. Model/LLM - The reasoning engine\n",
    "2. System Prompt - Instructions guiding behavior  \n",
    "3. Message History - Conversation context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your First Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')\n",
    "\n",
    "# Define system prompt\n",
    "system_prompt = \"You are a helpful assistant that provides concise and accurate responses.\"\n",
    "\n",
    "# Create agent\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the agent\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"What is machine learning?\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static model with configuration\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.5-flash',\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "agent = create_agent(model=model)\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Explain neural networks in one sentence\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed system prompt\n",
    "system_prompt = \"\"\"You are a financial analyst specializing in tech stocks.\n",
    "\n",
    "Guidelines:\n",
    "- Provide data-driven analysis\n",
    "- Keep responses concise (2-3 paragraphs max)\n",
    "- Present numbers with proper formatting ($XXX.XX)\n",
    "- Avoid speculation without data\n",
    "\"\"\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"What factors affect stock prices?\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Agent Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect message sequence\n",
    "response = agent.invoke({\n",
    "    'messages': [HumanMessage(\"Compare growth vs value stocks\")]\n",
    "})\n",
    "\n",
    "# Show message types and count\n",
    "for idx, msg in enumerate(response['messages']):\n",
    "    print(f\"{idx}. {type(msg).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role-Based Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer support agent\n",
    "support_agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=\"\"\"You are a friendly customer support agent.\n",
    "    \n",
    "    - Use simple language (avoid jargon)\n",
    "    - Ask clarifying questions when needed\n",
    "    - Maintain a warm, empathetic tone\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "response = support_agent.invoke({\n",
    "    'messages': [HumanMessage(\"I can't log into my account\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical expert agent\n",
    "tech_agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=\"\"\"You are a technical expert.\n",
    "    \n",
    "    - Provide detailed technical responses\n",
    "    - Use precise terminology\n",
    "    - Include code examples when relevant\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "response = tech_agent.invoke({\n",
    "    'messages': [HumanMessage(\"Explain REST API\")]\n",
    "})\n",
    "\n",
    "response['messages'][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Dynamic Model Selection"
  },
  {
   "cell_type": "markdown",
   "source": "## Key Takeaways\n\n- Agents combine models with reasoning capabilities\n- System prompts define agent behavior and personality\n- Model configuration controls response characteristics\n- Dynamic model selection enables cost optimization\n- Role-based prompts create specialized agents\n- Messages flow through the agent in a structured sequence",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test with short conversation (uses basic model)\nresponse = dynamic_agent.invoke({\n    'messages': [HumanMessage(\"What is AI?\")]\n})\n\nresponse['messages'][-1].text",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n\n# Define basic and advanced models\nbasic_model = ChatGoogleGenerativeAI(model='gemini-2.0-flash-exp')\nadvanced_model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')\n\n@wrap_model_call\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n    \"\"\"Choose model based on conversation complexity.\"\"\"\n    message_count = len(request.state[\"messages\"])\n    \n    if message_count > 10:\n        # Use advanced model for longer conversations\n        model = advanced_model\n    else:\n        # Use basic model for shorter conversations\n        model = basic_model\n    \n    return handler(request.override(model=model))\n\n# Create agent with dynamic model selection\ndynamic_agent = create_agent(\n    model=basic_model,  # Default model\n    middleware=[dynamic_model_selection]\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create an agent with a custom role and test it with different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}